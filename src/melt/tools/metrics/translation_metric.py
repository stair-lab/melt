"""
This module defines the TranslationMetric class, which evaluates the
quality of text translations using metrics such as BLEU and hLepor.

The module includes functions to compute these metrics based on the
predictions generated by a translation model and reference translations
provided in the input data.

Dependencies:
- evaluate
- hlepor
- BaseMetric (from .base)
- normalize_text (from .utils)
"""

from typing import Dict
try:
    import evaluate
except ImportError as e:
    raise ImportError("The 'evaluate' module is required but not installed.") from e

try:
    from hlepor import hlepor_score
except ImportError as e:
    raise ImportError("The 'hlepor' module is required but not installed.") from e
from .base import BaseMetric

from .utils import normalize_text



class TranslationMetric(BaseMetric):
    """Evaluate the quality of text translations using metrics like BLEU
    and hLepor."""

    def __init__(self, data, args) -> None:
        self.bleu_metrics = evaluate.load("bleu")
        super().__init__(data, args)

    def evaluate(self, data: Dict, args):
        """Computes the translation quality metrics for
        a set of predictions and references provided in the dictionary.

        Args:
            data (Dict): A dictionary expected to contain two keys:

                - predictions: A list of translated texts generated
                by the translation model.

                - references: A list of reference translations for evaluating
                the quality of the model's predictions.

        Returns:
            1. The original data dictionary, which contains the raw predictions
            and references.

            2. A result dictionary with the following keys:

                - "bleu": The computed BLEU score for the translations.

                - "hLepor": The computed hLepor score for the translations.
        """
        predictions = data["predictions"]
        references = data["references"]
        predictions = [self._get_answer(pre, args) for pre in predictions]
        references = [normalize_text(ref) for ref in references]

        bleu_score = self.bleu_metrics.compute(
            predictions=predictions, references=references
        )["bleu"]
        result = {
            "bleu": bleu_score,
            "hLepor": hlepor_score(references, predictions),
        }
        return data, result
